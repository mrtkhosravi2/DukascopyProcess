  currently we do download historical data from dukascopy. now we want to add live data support. the app gets data from dukascopy and exposes web services so that other local apps can access the data from our app.    
  data is provided as timeframes like S1, S5, etc. these timeframes are averaged values for the relative timespan; for example each 5 sec we will have a new value for S5. we'll have lookback_window latest values for    
  each timeframe. default number of lookback_window is 64 (see d:\Github\DukascopyProcess\config.ini for timeframes and lookback_window). so at any time we'll have lookback_window (64 by default) latest values for      
  each time frame (S1, S5, S10, etc.).                                                                                                                                                                                     
  obviously at the start of the app, some historical data should be collected to fill the values needed for timeframes, which can be huge (like 64 * 3600 for H1 data).                                                    
  each timeframe is devisible to previous timeframe. it is an invariant of our system. for example S5 is 5 times S1, S10 is two times S5. so we could easily aggregate timeframe data hierarchically, starting from S1     
  and moving upward.                                                                                                                                                                                                       
  timeframes are aligned by hour. for example S5 values should be aligned by the hour an we will always have a datapoint at xx:00:00, xx:00:05, etc. but will never have xx:00:03. Also another invariant of our system    
  is that timeframes can never be bigger than H1, so H2 , H3, etc. timeframes are forbidden.                                                                                                                               
  now let's talk about how timeframes are defined and calculated:                                                                                                                                                          
  1. S1 data is calculated from tick data. if there is no tick for a certain second, value is forward-filed from last valid tick data.                                                                                     
  2. S5 is calculated by averaging five S1 datapoints. it starts from a second divisible by 5. for example 04:27:05 to 04:27:09 (inclusive), 16:32:55 to 16:32:59, 11:43:20 to 11:43:24.                                   
  3. S10 is calculated by averaging to S5 values. for example with S5 data of 07:23:20 and 07:23:25 we can calc S10 for 07:23:20.                                                                                          
  4. higher timerfames are calculated the same way.                                                                                                                                                                        
  5. at all times, the app should expose lookback_window values per timeframes.                                                                                                                                                                                               
                                                                                                                                                                                                                           
  the connection is atrocious so disconnections may happen and handling these is important.                                                                                                                                                                                                           
  there is an important point about connection loss. when connection is lost, the new seconds will be NaN, not forward-filled from last valid point. higher timeframes related to these nan values will    
  also be nan obviously and these nan values propagate upward. so when nan values in S1 happen then new S5 values will become nan. new S10, S30, and higher also will be nan. downstream apps have no use  
  for incomplete data, so the web service responce should replace connection status with NaN existence in the buffer.                                                                                      
  on reconnection, app should fetch the missing data so that these nan values are replaced with actual values.                                                                                             
  for holidays there should be no values. the app should skip the whole day. so the first second is continued from 23:59:59 of the last open market day. closed market days other than weekends can be     
  identified by not having any tick between 00:00:00 and 02:00:00. so let's say we are in a close market day. if we have no tick in the span of 00:00:00 and 02:00:00, then this is a close market day. so we will   
  remove any data point we added for the current hour, essentially going back to the buffer at the start of the day. it makes sense to copy a backup of the buffer in case we need to revert the changes. 
  we make a flag named closeMarketDay as true and do not fetch data for the whole day.  
  data retrieval will resume on 00:00:00 of the next day. so in essence at the start of the day the flag will become on if it is a weekend. otherwise it will become off. 
  at 02:00:00 if there was no tick from the start of the day, the flage will be set as on. when the flage is on, no attempt to fetch the data is performed.

  Note: in production, our app should have both the current historical download functionality and the new live data exposure functionality we are going to develop. we should have a config to determine if only one of     
  these two should be run or both.                                                                                                                                                                                         
  note: only S1 timeframe has both mid price and spread values. other timeframes have only mid price.                                                                                                                      
  note: all timezone are in eet.                                                                                                                                                                                           
  we are talking about specs. don't jump into implementation. 
                                                                                                                                                                                       


  ---                                                                                                                                                                                                        Live Data Warmup Specification
                                                                                                                                                                                                           
  1. Configuration Parameters
  ┌────────────────────────┬────────────────────────────┬───────────────────────────────────────────────────────────────────────┐
  │       Parameter        │           Value            │                              Description                              │
  ├────────────────────────┼────────────────────────────┼───────────────────────────────────────────────────────────────────────┤
  │ timeframes             │ 1, 5, 10, 30, 60, 180, 900 │ Timeframe sizes in seconds                                            │
  ├────────────────────────┼────────────────────────────┼───────────────────────────────────────────────────────────────────────┤
  │ lookback_window        │ 64                         │ Number of datapoints to keep per timeframe                            │
  ├────────────────────────┼────────────────────────────┼───────────────────────────────────────────────────────────────────────┤
  │ largest_timeframe_secs │ 900 (M15)                  │ Largest configured timeframe                                          │
  ├────────────────────────┼────────────────────────────┼───────────────────────────────────────────────────────────────────────┤
  │ required_valid_hours   │ 17                         │ lookback_window // (3600 // largest_timeframe_secs) + 1 = 64 // 4 + 1 │
  └────────────────────────┴────────────────────────────┴───────────────────────────────────────────────────────────────────────┘
  ---
  2. Hour-by-Hour Tick Fetching

  2.1 Overview

  Fetch historical ticks starting from the current partial hour, then proceeding backwards hour-by-hour until 17 valid hours are accumulated.

  2.2 Definitions

  - Current hour: The EET hour when warmup started (e.g., if started at 05:23:32 EET, current hour = 05)
  - Valid hour: An hour that counts toward the 17-hour requirement
  - Tick range for hour H:
    - If H == current hour: [H:00:00, now - 1 second] (e.g., 05:00:00 to 05:23:31)
    - If H < current hour: [H:00:00, H:59:59] (full hour)

  2.3 Hour Validity Rules
  ┌─────────────────┬─────────────────────────────────────────────────────────────────────────────────┐
  │    Hour Type    │                               Validity Condition                                │
  ├─────────────────┼─────────────────────────────────────────────────────────────────────────────────┤
  │ Current hour    │ Valid if: (a) it has ≥1 tick, OR (b) the immediately preceding hour has ≥1 tick │
  ├─────────────────┼─────────────────────────────────────────────────────────────────────────────────┤
  │ All other hours │ Valid if: it has ≥1 tick                                                        │
  └─────────────────┴─────────────────────────────────────────────────────────────────────────────────┘
  Example scenario (starting at 05:23:32 EET):
  ┌──────────────┬────────────┬────────┬────────────────────────────┐
  │     Hour     │ Has Ticks? │ Valid? │           Reason           │
  ├──────────────┼────────────┼────────┼────────────────────────────┤
  │ 05 (current) │ No         │ Yes    │ Hour 04 has ticks          │
  ├──────────────┼────────────┼────────┼────────────────────────────┤
  │ 04           │ Yes        │ Yes    │ Has ticks                  │
  ├──────────────┼────────────┼────────┼────────────────────────────┤
  │ 03           │ No         │ No     │ No ticks, not current hour │
  ├──────────────┼────────────┼────────┼────────────────────────────┤
  │ 02           │ Yes        │ Yes    │ Has ticks                  │
  └──────────────┴────────────┴────────┴────────────────────────────┘
  2.4 Fetching Algorithm

  valid_hour_count = 0
  current_time = now()
  current_hour = floor(current_time to hour boundary)
  all_ticks = []
  hour_validity_map = {}

  # First pass: fetch ticks and determine raw tick presence
  hour = current_hour
  while valid_hour_count < 17:
      if hour == current_hour:
          start = hour:00:00
          end = current_time - 1 second
      else:
          start = hour:00:00
          end = hour:59:59

      ticks = fetch_ticks(start, end)
      hour_has_ticks = len(ticks) > 0
      hour_validity_map[hour] = hour_has_ticks

      if hour_has_ticks:
          all_ticks.append(ticks)

      # Determine if this hour is valid
      if hour == current_hour:
          # Current hour: valid if has ticks OR previous hour has ticks
          # (previous hour validity determined after fetching it)
          is_valid = hour_has_ticks  # tentative, may upgrade later
      else:
          is_valid = hour_has_ticks

          # Check if we need to upgrade current hour validity
          if hour == current_hour - 1 hour AND hour_has_ticks:
              if NOT hour_validity_map[current_hour]:
                  # Upgrade current hour to valid
                  valid_hour_count += 1  # retroactively count current hour

      if is_valid:
          valid_hour_count += 1

      hour = hour - 1 hour

  2.5 Output of Step 2

  - all_ticks[]: Array of tick data for all fetched hours (including invalid hours)
  - valid_hours[]: Set of hour timestamps that are considered valid
  - Count: exactly 17 valid hours

  ---
  3. S1 Data Generation

  3.1 Overview

  Convert raw ticks into per-second (S1) datapoints. Each second has exactly one datapoint.

  3.2 Algorithm

  Phase A: Generate S1 for ALL fetched hours (including invalid)

  For each fetched hour:
  for second in [hour:00:00 to hour:59:59]:  # 3600 seconds
      ticks_in_second = filter(all_ticks, timestamp == second)

      if len(ticks_in_second) > 0:
          s1_mid = average(tick.mid for tick in ticks_in_second)
          s1_spread = average(tick.spread for tick in ticks_in_second)
      else:
          s1_mid = previous_second.mid      # forward-fill
          s1_spread = previous_second.spread

      s1_data[second] = {mid: s1_mid, spread: s1_spread}

  Note on forward-fill edge case: For the very first second of the earliest fetched hour, if no tick exists, use the first available tick value from later in that hour (or subsequent hours).

  Phase B: Remove S1 data for invalid hours

  for hour in all_fetched_hours:
      if hour NOT in valid_hours:
          remove s1_data[hour:00:00 to hour:59:59]

  3.3 Output of Step 3

  - s1_data[]: Array of S1 datapoints
  - Each valid hour contributes exactly 3600 S1 datapoints
  - Total S1 datapoints = 17 × 3600 = 61,200 datapoints
  - Invalid hours have 0 S1 datapoints

  ---
  4. Higher Timeframe Calculation

  4.1 Timeframe Hierarchy
  ┌───────────┬─────────┬────────┬───────┬─────────────────┐
  │ Timeframe │ Seconds │ Parent │ Ratio │ Datapoints/Hour │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ S1        │ 1       │ —      │ —     │ 3600            │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ S5        │ 5       │ S1     │ 5     │ 720             │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ S10       │ 10      │ S5     │ 2     │ 360             │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ S30       │ 30      │ S10    │ 3     │ 120             │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ M1        │ 60      │ S30    │ 2     │ 60              │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ M3        │ 180     │ M1     │ 3     │ 20              │
  ├───────────┼─────────┼────────┼───────┼─────────────────┤
  │ M15       │ 900     │ M3     │ 5     │ 4               │
  └───────────┴─────────┴────────┴───────┴─────────────────┘
  4.2 Alignment Rule

  All higher timeframes are hour-aligned. Every hour boundary (HH:00:00) is always a datapoint for all timeframes.

  Examples for hour 05:
  - S5: 05:00:00, 05:00:05, 05:00:10, ..., 05:59:55
  - S10: 05:00:00, 05:00:10, 05:00:20, ..., 05:59:50
  - S30: 05:00:00, 05:00:30, 05:01:00, ..., 05:59:30
  - M1: 05:00:00, 05:01:00, 05:02:00, ..., 05:59:00
  - M3: 05:00:00, 05:03:00, 05:06:00, ..., 05:57:00
  - M15: 05:00:00, 05:15:00, 05:30:00, 05:45:00

  4.3 Calculation Algorithm

  Each higher timeframe is calculated exclusively from its parent timeframe:

  for timeframe in [S5, S10, S30, M1, M3, M15]:  # in order
      parent = timeframe.getParent()
      ratio = timeframe.seconds / parent.seconds

      for each valid_hour:
          for period_start in [hour:00:00, step by timeframe.seconds]:
              parent_values = get_parent_values(
                  parent_data,
                  period_start,
                  period_start + timeframe.seconds - 1
              )
              # parent_values contains exactly `ratio` values

              timeframe_data[period_start] = average(parent_values)

  Example: S5 at 05:00:00
  S5[05:00:00] = average(S1[05:00:00], S1[05:00:01], S1[05:00:02], S1[05:00:03], S1[05:00:04])

  Example: S10 at 05:00:00
  S10[05:00:00] = average(S5[05:00:00], S5[05:00:05])

  Example: M15 at 05:00:00
  M15[05:00:00] = average(M3[05:00:00], M3[05:03:00], M3[05:06:00], M3[05:09:00], M3[05:12:00])

  4.4 Invalid Hour Handling

  Higher timeframes skip invalid hours entirely. If an hour is invalid:
  - No datapoints are generated for that hour
  - The calculation continues with the next valid hour's data

  ---
  5. Ring Buffer Population

  5.1 Overview

  After all timeframe data is calculated, populate the ring buffers with the last lookback_window (64) datapoints for each timeframe.

  5.2 Buffer Sizes
  ┌───────────┬───────────────────────┬──────────────────┬─────────────┐
  │ Timeframe │ Datapoints/Valid Hour │ Total (17 hours) │ Buffer Size │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ S1        │ 3600                  │ 61,200           │ 64          │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ S5        │ 720                   │ 12,240           │ 64          │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ S10       │ 360                   │ 6,120            │ 64          │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ S30       │ 120                   │ 2,040            │ 64          │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ M1        │ 60                    │ 1,020            │ 64          │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ M3        │ 20                    │ 340              │ 64          │
  ├───────────┼───────────────────────┼──────────────────┼─────────────┤
  │ M15       │ 4                     │ 68               │ 64          │
  └───────────┴───────────────────────┴──────────────────┴─────────────┘
  5.3 Population Algorithm

  for each instrument:
      for each timeframe:
          sorted_data = sort_by_timestamp(timeframe_data)
          last_64 = sorted_data[-64:]  # take last 64 datapoints

          ring_buffer[instrument][timeframe].populate(last_64)

  ---
  6. Edge Cases & Constraints
  ┌──────────────────────────────────────────────┬───────────────────────────────────────────────────────────────────────────────────────────────────┐
  │                   Scenario                   │                                             Handling                                              │
  ├──────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Market closed (weekend/holiday)              │ Keep fetching backwards until 17 valid hours found                                                │
  ├──────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Current hour is first hour after market open │ If current hour has no ticks but previous valid market hour has ticks, current hour becomes valid │
  ├──────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Gap between valid hours                      │ Invalid hours are skipped; no interpolation                                                       │
  ├──────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Very old data needed                         │ Continue fetching as far back as necessary                                                        │
  ├──────────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Current partial hour has <1 second elapsed   │ Fetch 0 ticks for current hour; validity depends on previous hour                                 │
  └──────────────────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────┘
  ---
  7. Data Flow Summary

  ┌─────────────────┐
  │  Fetch Ticks    │  Hour-by-hour until 17 valid hours
  │  (Step 2)       │
  └────────┬────────┘
           │ Raw ticks + valid_hours[]
           ▼
  ┌─────────────────┐
  │  Generate S1    │  3600 datapoints per valid hour
  │  (Step 3)       │  Forward-fill gaps
  └────────┬────────┘
           │ S1 data (valid hours only)
           ▼
  ┌─────────────────┐
  │  Calculate      │  S5 ← S1, S10 ← S5, ... M15 ← M3
  │  Timeframes     │  All hour-aligned
  │  (Step 4)       │
  └────────┬────────┘
           │ All timeframe data
           ▼
  ┌─────────────────┐
  │  Populate       │  Last 64 datapoints per timeframe
  │  Ring Buffers   │
  │  (Step 5)       │
  └─────────────────┘


========================================================================================================================

ok. the warmup sequence for live processing works fine. now we want to add live data processing functionality. for now we are interested in drafting specs, not implementation.
we already have a ring buffer that holds the last lookback_window (default to 64) initialized during warmup procedure. 
the live data processing starts after warmup is finished.  live data is fetched every second and this updates the ring buffer.

the connection is atrocious so disconnections may happen and handling these is important.                                                                                                                                                                                                           

there is an important point about connection loss. when connection is lost, the new seconds will be -1, not forward-filled from last valid point. higher timeframes 
related to these -1 values will also be -1 obviously and these -1 values propagate upward. so when -1 values in S1 happen then new S5 values will become -1. new S10, 
S30, and higher also will be -1. downstream apps have no use for incomplete data, so the websocket service should have a NaN existence flag in the buffer.
on reconnection, app should fetch the missing data so that these -1 values are replaced with actual values. Any value below zero is invalid for midprice and spread
so a simple check of value < 0 will handle the price being NaN (we use -1 instead of actual NaN to avoid compatibility issues with downstream apps).

for holidays, weekends, and  new values should not be produced. so the first second is continued from 23:59:59 of the last open market day. closed market days other than 
weekends (holidays and other close market days) can be identified by not having any tick between 00:00:00 and 02:00:00. so let's say we are in a close market day. if we have 
no tick in the span of 00:00:00 and 02:00:00, then this is a close market day. so we will remove any data point we added for the current hour, essentially going back 
to the buffer at the start of the day. it makes sense to copy a backup of the buffer in case we need to revert the changes. this makes the status of the day (open or close) 
in the time span of 00:00:00 to 02:00:00 acceptable since no downstream app uses live data in that period. after 02 hour we definitely know if this is a close or open
market day. so it makes sense to assume it is and open day unless we have no tick in the aforementioned period. in that case we could revert to the backup buffer ring 
we took at the start of day. if the app startup was in the range of 00 to 02 hour, the situation may get complicated, so if the program startup is within this period,
raise an error and quit the app, clearly stating that we should start the app after 02. 

in the case of closed market day, we set a flag named closeMarketDay as true and do not fetch data for the whole day.  
data retrieval will resume on 00:00:00 of the next day. so in essence at the start of the day the flag will become on if it is a weekend. otherwise it will become off. 
at 02:00:00 if there was no tick from the start of the day, the flag will be set as on. when the flag is on, no attempt to fetch the data is performed.


note: All timeframes have only mid price: (Ask + Bid) / 2.                                                                                                                      
note: Any and all times are based on EET timezone.

we are trying to perfect our specs. don't jump into implementation. 
Ask any questions you feel right to better clarify the specs.

===========================================================================================================================================

● Live Data Processing Specification                                                                                                                                                                       

  1. Overview                                                                                                                                                                                                 
  Live data processing subscribes to Dukascopy's WebSocket tick stream, aggregates ticks per second, and updates the ring buffer. Broadcasting to downstream clients is handled by existing code—this spec 
  covers only the ring buffer update logic.

  2. Prerequisites

  - Warmup procedure must complete successfully before live processing begins
  - Ring buffer is initialized with lookback_window seconds (default: 64) of historical data
  - App must NOT start between 00:00:00 and 02:00:00 EET (exit with error if attempted)

  3. Market Hours
  ┌─────────────────────┬──────────────────────────────────────┐
  │         Day         │                Status                │
  ├─────────────────────┼──────────────────────────────────────┤
  │ Monday 00:00:00 EET │ Market opens                         │
  ├─────────────────────┼──────────────────────────────────────┤
  │ Friday 23:59:59 EET │ Market closes                        │
  ├─────────────────────┼──────────────────────────────────────┤
  │ Saturday            │ Closed                               │
  ├─────────────────────┼──────────────────────────────────────┤
  │ Sunday              │ Closed                               │
  ├─────────────────────┼──────────────────────────────────────┤
  │ Holidays            │ Detected dynamically (see Section 7) │
  └─────────────────────┴──────────────────────────────────────┘
  4. Data Flow

  Dukascopy WebSocket Tick Stream
              │
              ▼
      Tick Aggregator
      (group by second, calculate mid & spread)
              │
              ▼
      S1 Data Point
      {timestamp, mid, spread}
              │
              ▼
      Ring Buffer Update
      (S1 inserted, higher timeframes recalculated)

  4.1 Tick Aggregation

  For each second:
  - Collect all ticks received within that second
  - Calculate: mid = (ask + bid) / 2 (averaged across ticks)
  - Calculate: spread = ask - bid (averaged across ticks)
  - If no ticks received for a second during market hours with active connection: forward-fill from previous second

  4.2 Ring Buffer Update

  At each second boundary:
  1. Insert new S1 data point into ring buffer
  2. Higher timeframes (S5, S10, S30, M1, etc.) are derived from S1 data in the buffer
  3. If any S1 component of a higher timeframe is -1, that entire higher timeframe value becomes -1

  5. State Variables
  ┌─────────────────────┬────────────┬───────────────────────────────────────────────────┐
  │      Variable       │    Type    │                    Description                    │
  ├─────────────────────┼────────────┼───────────────────────────────────────────────────┤
  │ closeMarketDay      │ boolean    │ True if today is a non-trading day                │
  ├─────────────────────┼────────────┼───────────────────────────────────────────────────┤
  │ connected           │ boolean    │ True if WebSocket connection is active            │
  ├─────────────────────┼────────────┼───────────────────────────────────────────────────┤
  │ lastValidTimestamp  │ timestamp  │ Last second with valid data                       │
  ├─────────────────────┼────────────┼───────────────────────────────────────────────────┤
  │ disconnectTimestamp │ timestamp  │ When connection was lost (null if connected)      │
  ├─────────────────────┼────────────┼───────────────────────────────────────────────────┤
  │ bufferBackup        │ RingBuffer │ Copy of ring buffer taken at 00:00:00 EET         │
  ├─────────────────────┼────────────┼───────────────────────────────────────────────────┤
  │ hasNaN              │ boolean    │ Global flag: true if any -1 exists in ring buffer │
  └─────────────────────┴────────────┴───────────────────────────────────────────────────┘
  6. Connection Management

  6.1 Initial Connection (after warmup)

  1. Subscribe to Dukascopy WebSocket tick stream for all configured instruments
  2. Set connected = true
  3. Begin tick aggregation and ring buffer updates

  6.2 Disconnection Detected

  When connection is lost:
  1. Set connected = false
  2. Record disconnectTimestamp = now
  3. For each new second while disconnected:
    - Insert -1 for mid and spread into ring buffer
    - All affected higher timeframes become -1
    - Set hasNaN = true
  4. Attempt reconnection every second (never give up)

  6.3 Reconnection

  When connection is restored:
  1. Calculate gap = now - disconnectTimestamp
  2. If gap < 1 minute:
    - Fetch missing seconds via historical API
    - Replace -1 values in ring buffer with actual data
    - Recalculate affected higher timeframes
    - If all -1 values replaced: set hasNaN = false
  3. If gap ≥ 1 minute:
    - Perform full warmup procedure
    - Resume live processing after warmup completes
  4. Set connected = true
  5. Clear disconnectTimestamp

  7. Closed Market Day Handling

  7.1 Daily Routine at 00:00:00 EET

  At 00:00:00 EET each day:
  │
  ├─ Is Saturday or Sunday?
  │   ├─ YES → Set closeMarketDay = true, skip data fetch for entire day
  │   └─ NO  → Set closeMarketDay = false
  │           Take bufferBackup = copy of current ring buffer
  │           Begin normal data fetch

  7.2 Holiday Detection at 02:00:00 EET

  At 02:00:00 EET (if closeMarketDay == false):
  │
  ├─ Any ticks received since 00:00:00?
  │   ├─ YES → Confirmed trading day, continue normal operation
  │   └─ NO  → Holiday detected:
  │            1. Restore ring buffer from bufferBackup
  │            2. Set closeMarketDay = true
  │            3. Stop data fetch for remainder of day

  7.3 Resumption

  - Data fetch resumes at 00:00:00 EET of the next day
  - Process repeats from Section 7.1

  8. Invalid Data Handling

  8.1 Invalid Value Representation

  - Invalid mid: -1
  - Invalid spread: -1
  - Check: value < 0 indicates invalid data

  8.2 Propagation Rules
  ┌────────────────────────┬──────────────────────────────────────────────────────┐
  │        Scenario        │                        Result                        │
  ├────────────────────────┼──────────────────────────────────────────────────────┤
  │ S1 is -1               │ All higher timeframes containing this S1 become -1   │
  ├────────────────────────┼──────────────────────────────────────────────────────┤
  │ 1 of 5 S1 values is -1 │ Entire S5 is -1                                      │
  ├────────────────────────┼──────────────────────────────────────────────────────┤
  │ Connection lost        │ All new values are -1 until reconnected and repaired │
  └────────────────────────┴──────────────────────────────────────────────────────┘
  8.3 hasNaN Flag

  - Set to true when any -1 value exists in ring buffer
  - Set to false only when all -1 values are replaced with valid data
  - Downstream clients can check this flag before consuming data

  9. Edge Cases
  ┌───────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────────┐
  │                 Scenario                  │                                          Behavior                                           │
  ├───────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────┤
  │ App starts between 00:00-02:00 EET        │ Exit with error message: "Cannot start between 00:00 and 02:00 EET"                         │
  ├───────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Disconnection spans midnight              │ Handle day transition during reconnection; if new day is weekend, set closeMarketDay = true │
  ├───────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Disconnection during 00:00-02:00 period   │ On reconnection, if no ticks found for the day so far and time ≥ 02:00, treat as holiday    │
  ├───────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Connection never established after warmup │ Keep retrying every second; ring buffer continues to fill with -1                           │
  ├───────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────┤
  │ Historical API fails during repair        │ Keep -1 values; set hasNaN = true; continue live processing                                 │
  └───────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────┘
  10. Sequence Diagrams

  10.1 Normal Operation

  Time    Action
  ─────   ──────────────────────────────────────
  14:30:00.000  Second boundary reached
  14:30:00.050  Aggregate ticks from 14:29:59
  14:30:00.051  Insert S1 into ring buffer
  14:30:00.052  Recalculate higher timeframes
  14:30:00.053  Update hasNaN flag if needed
          ...   (existing broadcast code sends to clients)
  14:30:01.000  Next second boundary...

  10.2 Disconnection and Recovery (<1 min)

  Time        Action
  ─────       ──────────────────────────────────────
  14:30:00    Connection lost, disconnectTimestamp = 14:30:00
  14:30:01    Insert -1, hasNaN = true, retry connection
  14:30:02    Insert -1, retry connection
  ...
  14:30:45    Connection restored (gap = 45 seconds)
  14:30:45    Fetch historical data for 14:30:00 - 14:30:44
  14:30:45    Replace -1 values with actual data
  14:30:45    Recalculate higher timeframes
  14:30:45    hasNaN = false (if all repaired)
  14:30:46    Resume normal operation

  10.3 Disconnection and Recovery (≥1 min)

  Time        Action
  ─────       ──────────────────────────────────────
  14:30:00    Connection lost
  14:30:01    Insert -1, retry...
  ...
  14:35:00    Connection restored (gap = 5 minutes)
  14:35:00    Initiate full warmup procedure
  14:37:00    Warmup complete, ring buffer rebuilt
  14:37:00    Resume live processing

  11. Configuration Parameters
  ┌────────────────────────┬──────────────┬────────────────────────────────────────────────────┐
  │       Parameter        │   Default    │                    Description                     │
  ├────────────────────────┼──────────────┼────────────────────────────────────────────────────┤
  │ lookback_window        │ 64           │ Ring buffer size in seconds                        │
  ├────────────────────────┼──────────────┼────────────────────────────────────────────────────┤
  │ reconnect_interval     │ 1 second     │ Time between reconnection attempts                 │
  ├────────────────────────┼──────────────┼────────────────────────────────────────────────────┤
  │ repair_threshold       │ 60 seconds   │ Gap threshold: below = repair, above = full warmup │
  ├────────────────────────┼──────────────┼────────────────────────────────────────────────────┤
  │ holiday_detection_hour │ 02:00:00 EET │ Time to confirm holiday status                     │
  └────────────────────────┴──────────────┴────────────────────────────────────────────────────┘
  12. Open Items

  1. Historical API for repair: Need to confirm Dukascopy API endpoint for fetching missed tick data (investigate JForex historical data methods)